{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pgavhxo8E2OQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bibliothèques importées avec succès.\n",
            "Données d'entraînement, de validation, de test et mappings chargés avec succès.\n",
            "Train DataFrame shape: (25600162, 6)\n",
            "Validation DataFrame shape: (3200021, 6)\n",
            "Test DataFrame shape: (3200021, 6)\n",
            "Nombre d'utilisateurs mappés : 200948\n",
            "Nombre de films mappés : 84432\n",
            "\n",
            "--- Modèle 1 : Filtrage Collaboratif Basé sur la Similarité ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abdel\\OneDrive\\Documents\\GitHub\\DeepRec\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:143: RuntimeWarning: overflow encountered in scalar multiply\n",
            "  num_cells = num_rows * num_columns\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "negative dimensions are not allowed",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Modèle 1 : Filtrage Collaboratif Basé sur la Similarité ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Pour une implémentation simple, nous allons créer une matrice utilisateur-film\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# à partir de l'ensemble d'entraînement.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Créer une matrice pivot (utilisateur x film) à partir du DataFrame d'entraînement\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Remplir les valeurs manquantes avec NaN\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m user_movie_matrix = \u001b[43mtrain_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser_id_mapped\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmovie_id_mapped\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrating\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAperçu de la matrice utilisateur-film (sparse) :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(user_movie_matrix.head())\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdel\\OneDrive\\Documents\\GitHub\\DeepRec\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:9346\u001b[39m, in \u001b[36mDataFrame.pivot\u001b[39m\u001b[34m(self, columns, index, values)\u001b[39m\n\u001b[32m   9339\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   9340\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mpivot\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   9341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpivot\u001b[39m(\n\u001b[32m   9342\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, columns, index=lib.no_default, values=lib.no_default\n\u001b[32m   9343\u001b[39m ) -> DataFrame:\n\u001b[32m   9344\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpivot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[32m-> \u001b[39m\u001b[32m9346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdel\\OneDrive\\Documents\\GitHub\\DeepRec\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:570\u001b[39m, in \u001b[36mpivot\u001b[39m\u001b[34m(data, columns, index, values)\u001b[39m\n\u001b[32m    566\u001b[39m         indexed = data._constructor_sliced(data[values]._values, index=multiindex)\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m result = \u001b[43mindexed\u001b[49m\u001b[43m.\u001b[49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns_listlike\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    571\u001b[39m result.index.names = [\n\u001b[32m    572\u001b[39m     name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m result.index.names\n\u001b[32m    573\u001b[39m ]\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdel\\OneDrive\\Documents\\GitHub\\DeepRec\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4626\u001b[39m, in \u001b[36mSeries.unstack\u001b[39m\u001b[34m(self, level, fill_value, sort)\u001b[39m\n\u001b[32m   4581\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4582\u001b[39m \u001b[33;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[32m   4583\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4622\u001b[39m \u001b[33;03mb    2    4\u001b[39;00m\n\u001b[32m   4623\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[32m-> \u001b[39m\u001b[32m4626\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdel\\OneDrive\\Documents\\GitHub\\DeepRec\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:517\u001b[39m, in \u001b[36munstack\u001b[39m\u001b[34m(obj, level, fill_value, sort)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj.dtype):\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value, sort=sort)\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m unstacker = \u001b[43m_Unstacker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_constructor_expanddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker.get_result(\n\u001b[32m    521\u001b[39m     obj._values, value_columns=\u001b[38;5;28;01mNone\u001b[39;00m, fill_value=fill_value\n\u001b[32m    522\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdel\\OneDrive\\Documents\\GitHub\\DeepRec\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:154\u001b[39m, in \u001b[36m_Unstacker.__init__\u001b[39m\u001b[34m(self, index, level, constructor, sort)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_cells > np.iinfo(np.int32).max:\n\u001b[32m    147\u001b[39m     warnings.warn(\n\u001b[32m    148\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cells \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    149\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min the resulting pandas object.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    150\u001b[39m         PerformanceWarning,\n\u001b[32m    151\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    152\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdel\\OneDrive\\Documents\\GitHub\\DeepRec\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:206\u001b[39m, in \u001b[36m_Unstacker._make_selectors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m.full_shape = ngroups, stride\n\u001b[32m    205\u001b[39m selector = \u001b[38;5;28mself\u001b[39m.sorted_labels[-\u001b[32m1\u001b[39m] + stride * comp_index + \u001b[38;5;28mself\u001b[39m.lift\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m mask = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfull_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m mask.put(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.sum() < \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.index):\n",
            "\u001b[31mValueError\u001b[39m: negative dimensions are not allowed"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "# 2_Baseline_Models.ipynb\n",
        "\n",
        "Ce notebook implémente et évalue des modèles de recommandation de base :\n",
        "le filtrage collaboratif (basé sur la similarité) et la factorisation matricielle (SVD).\n",
        "Ces modèles serviront de points de comparaison pour le modèle de Deep Learning.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Importation des bibliothèques nécessaires\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Pour le filtrage collaboratif basé sur la similarité\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Pour la factorisation matricielle (SVD)\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "print(\"Bibliothèques importées avec succès.\")\n",
        "\n",
        "# 2. Chargement des données prétraitées et des mappings\n",
        "try:\n",
        "    train_df = pd.read_csv('../data/train_ratings.csv')\n",
        "    val_df = pd.read_csv('../data/val_ratings.csv')\n",
        "    test_df = pd.read_csv('../data/test_ratings.csv')\n",
        "\n",
        "    user_to_id = np.load('../data/user_to_id.npy', allow_pickle=True).item()\n",
        "    id_to_user = np.load('../data/id_to_user.npy', allow_pickle=True).item()\n",
        "    movie_to_id = np.load('../data/movie_to_id.npy', allow_pickle=True).item()\n",
        "    id_to_movie = np.load('../data/id_to_movie.npy', allow_pickle=True).item()\n",
        "\n",
        "    print(\"Données d'entraînement, de validation, de test et mappings chargés avec succès.\")\n",
        "    print(f\"Train DataFrame shape: {train_df.shape}\")\n",
        "    print(f\"Validation DataFrame shape: {val_df.shape}\")\n",
        "    print(f\"Test DataFrame shape: {test_df.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Erreur : Les fichiers de données ou de mappings n'ont pas été trouvés.\")\n",
        "    print(\"Assure-toi d'avoir exécuté le notebook '1_EDA_Preprocessing.ipynb' au préalable.\")\n",
        "    exit()\n",
        "\n",
        "# Déterminer le nombre total d'utilisateurs et de films mappés\n",
        "n_users_mapped = len(user_to_id)\n",
        "n_movies_mapped = len(movie_to_id)\n",
        "print(f\"Nombre d'utilisateurs mappés : {n_users_mapped}\")\n",
        "print(f\"Nombre de films mappés : {n_movies_mapped}\")\n",
        "\n",
        "# --- Modèle 1 : Filtrage Collaboratif Basé sur la Similarité (Simple) ---\n",
        "print(\"\\n--- Modèle 1 : Filtrage Collaboratif Basé sur la Similarité ---\")\n",
        "\n",
        "# Pour une implémentation simple, nous allons créer une matrice utilisateur-film\n",
        "# à partir de l'ensemble d'entraînement.\n",
        "\n",
        "# Créer une matrice pivot (utilisateur x film) à partir du DataFrame d'entraînement\n",
        "# Remplir les valeurs manquantes avec NaN\n",
        "user_movie_matrix = train_df.pivot(index='user_id_mapped', columns='movie_id_mapped', values='rating')\n",
        "print(\"\\nAperçu de la matrice utilisateur-film (sparse) :\")\n",
        "print(user_movie_matrix.head())\n",
        "\n",
        "# Calcul de la similarité entre utilisateurs (basée sur les notes communes)\n",
        "# Remplir les NaN avec 0 pour le calcul de similarité (ou une autre stratégie)\n",
        "# Attention : Remplir avec 0 peut fausser la similarité si 0 est une note possible.\n",
        "# Pour la similarité cosinus, il est souvent préférable de ne considérer que les éléments notés en commun.\n",
        "# Pour simplifier ici, nous allons utiliser une approche directe sur la matrice remplie.\n",
        "user_movie_matrix_filled = user_movie_matrix.fillna(0)\n",
        "user_similarity = cosine_similarity(user_movie_matrix_filled)\n",
        "user_similarity_df = pd.DataFrame(user_similarity, index=user_movie_matrix.index, columns=user_movie_matrix.index)\n",
        "print(\"\\nAperçu de la matrice de similarité utilisateur :\")\n",
        "print(user_similarity_df.head())\n",
        "\n",
        "# Fonction de prédiction simple pour le filtrage collaboratif basé sur l'utilisateur\n",
        "def predict_user_based_cf(user_id, movie_id, user_movie_matrix, user_similarity_df, k=10):\n",
        "    \"\"\"\n",
        "    Prédit la note d'un film pour un utilisateur donné en utilisant le filtrage collaboratif basé sur l'utilisateur.\n",
        "    Args:\n",
        "        user_id (int): ID mappé de l'utilisateur.\n",
        "        movie_id (int): ID mappé du film.\n",
        "        user_movie_matrix (pd.DataFrame): Matrice utilisateur-film.\n",
        "        user_similarity_df (pd.DataFrame): Matrice de similarité utilisateur.\n",
        "        k (int): Nombre de voisins les plus proches à considérer.\n",
        "    Returns:\n",
        "        float: Prédiction de la note.\n",
        "    \"\"\"\n",
        "    if movie_id not in user_movie_matrix.columns:\n",
        "        return user_movie_matrix.mean().mean() # Retourne la moyenne générale si le film est inconnu\n",
        "\n",
        "    # Trouver les utilisateurs similaires qui ont noté le film\n",
        "    similar_users = user_similarity_df[user_id].drop(user_id).sort_values(ascending=False)\n",
        "\n",
        "    # Filtrer les utilisateurs qui ont noté le film en question\n",
        "    rated_by_similar = user_movie_matrix[movie_id].dropna()\n",
        "\n",
        "    # Trouver les k voisins les plus proches qui ont noté le film\n",
        "    # On prend les 'k' utilisateurs les plus similaires qui ont effectivement noté le film\n",
        "    neighbors = similar_users.index.intersection(rated_by_similar.index)\n",
        "    top_k_neighbors = similar_users[neighbors].head(k).index\n",
        "\n",
        "    if not top_k_neighbors.empty:\n",
        "        # Calcul de la moyenne pondérée des notes des voisins\n",
        "        # Poids = similarité, Valeur = note du film par le voisin\n",
        "        numerator = sum(user_similarity_df.loc[user, user_id] * user_movie_matrix.loc[user, movie_id]\n",
        "                        for user in top_k_neighbors)\n",
        "        denominator = sum(abs(user_similarity_df.loc[user, user_id]) for user in top_k_neighbors)\n",
        "\n",
        "        if denominator == 0:\n",
        "            return user_movie_matrix.loc[user_id].mean() if user_id in user_movie_matrix.index else user_movie_matrix.mean().mean()\n",
        "\n",
        "        prediction = numerator / denominator\n",
        "        # S'assurer que la prédiction est dans la plage des notes (1 à 5)\n",
        "        return max(1.0, min(5.0, prediction))\n",
        "    else:\n",
        "        # Si aucun voisin n'a noté le film, retourner la moyenne de l'utilisateur ou la moyenne générale\n",
        "        return user_movie_matrix.loc[user_id].mean() if user_id in user_movie_matrix.index else user_movie_matrix.mean().mean()\n",
        "\n",
        "# Évaluation du modèle de filtrage collaboratif sur l'ensemble de test\n",
        "print(\"\\nÉvaluation du Filtrage Collaboratif Basé sur la Similarité (sur l'ensemble de test)...\")\n",
        "predictions_cf = []\n",
        "true_ratings_cf = []\n",
        "\n",
        "# Pour des raisons de performance, évaluer sur un sous-ensemble du test_df si test_df est très grand\n",
        "# Ou optimiser la fonction de prédiction pour des calculs par lots.\n",
        "# Ici, nous allons itérer, ce qui peut être lent pour de grands datasets.\n",
        "# Limiter à 1000 prédictions pour l'exemple si le dataset est grand.\n",
        "sample_test_df = test_df.sample(n=min(len(test_df), 5000), random_state=42) # Limiter à 5000 échantillons\n",
        "\n",
        "for index, row in sample_test_df.iterrows():\n",
        "    user_id = row['user_id_mapped']\n",
        "    movie_id = row['movie_id_mapped']\n",
        "    true_rating = row['rating']\n",
        "\n",
        "    predicted_rating = predict_user_based_cf(user_id, movie_id, user_movie_matrix, user_similarity_df)\n",
        "    predictions_cf.append(predicted_rating)\n",
        "    true_ratings_cf.append(true_rating)\n",
        "\n",
        "# Calcul des métriques\n",
        "rmse_cf = sqrt(mean_squared_error(true_ratings_cf, predictions_cf))\n",
        "mae_cf = mean_absolute_error(true_ratings_cf, predictions_cf)\n",
        "\n",
        "print(f\"RMSE (Filtrage Collaboratif) : {rmse_cf:.4f}\")\n",
        "print(f\"MAE (Filtrage Collaboratif) : {mae_cf:.4f}\")\n",
        "\n",
        "\n",
        "# --- Modèle 2 : Factorisation Matricielle (SVD) avec Surprise ---\n",
        "print(\"\\n--- Modèle 2 : Factorisation Matricielle (SVD) avec Surprise ---\")\n",
        "\n",
        "# Charger les données dans le format attendu par Surprise\n",
        "# Le Reader spécifie la plage des notes\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Diviser les données en entraînement et test pour Surprise\n",
        "# Surprise a sa propre fonction de split qui est plus adaptée à ses objets Dataset.\n",
        "# Nous allons utiliser l'ensemble d'entraînement complet pour entraîner le modèle SVD\n",
        "# et l'évaluer sur l'ensemble de test que nous avons déjà préparé.\n",
        "# Pour Surprise, il est plus simple de recharger les données et de faire un split interne.\n",
        "# Ou alors, on peut construire un Dataset à partir de nos train_df et test_df.\n",
        "\n",
        "# Créer un Dataset Surprise à partir de train_df\n",
        "trainset = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader).build_full_trainset()\n",
        "\n",
        "# Créer un testset Surprise à partir de test_df\n",
        "# Le format de testset pour Surprise est une liste de tuples (userId, movieId, true_rating)\n",
        "testset = list(test_df.apply(lambda x: (x['userId'], x['movieId'], x['rating']), axis=1))\n",
        "\n",
        "print(f\"Taille du trainset Surprise : {trainset.n_ratings} ratings\")\n",
        "print(f\"Taille du testset Surprise : {len(testset)} ratings\")\n",
        "\n",
        "# Entraînement du modèle SVD\n",
        "# On peut ajuster les paramètres comme n_factors (nombre de facteurs latents), n_epochs, lr_all, reg_all\n",
        "print(\"Entraînement du modèle SVD...\")\n",
        "algo_svd = SVD(n_factors=50, n_epochs=20, random_state=42, verbose=True)\n",
        "algo_svd.fit(trainset)\n",
        "print(\"Modèle SVD entraîné avec succès.\")\n",
        "\n",
        "# Évaluation du modèle SVD sur l'ensemble de test\n",
        "print(\"\\nÉvaluation du modèle SVD (sur l'ensemble de test)...\")\n",
        "predictions_svd = algo_svd.test(testset)\n",
        "\n",
        "# Calcul des métriques avec Surprise\n",
        "rmse_svd = accuracy.rmse(predictions_svd, verbose=False)\n",
        "mae_svd = accuracy.mae(predictions_svd, verbose=False)\n",
        "\n",
        "print(f\"RMSE (SVD) : {rmse_svd:.4f}\")\n",
        "print(f\"MAE (SVD) : {mae_svd:.4f}\")\n",
        "\n",
        "# --- Comparaison des Modèles ---\n",
        "print(\"\\n--- Comparaison des performances des modèles de base ---\")\n",
        "results = pd.DataFrame({\n",
        "    'Modèle': ['Filtrage Collaboratif', 'SVD'],\n",
        "    'RMSE': [rmse_cf, rmse_svd],\n",
        "    'MAE': [mae_cf, mae_svd]\n",
        "})\n",
        "print(results)\n",
        "\n",
        "# Visualisation de la comparaison\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Modèle', y='RMSE', data=results, palette='coolwarm')\n",
        "plt.title('Comparaison des RMSE des modèles de base')\n",
        "plt.ylabel('RMSE')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Modèle', y='MAE', data=results, palette='coolwarm')\n",
        "plt.title('Comparaison des MAE des modèles de base')\n",
        "plt.ylabel('MAE')\n",
        "plt.show()\n",
        "\n",
        "# --- Sauvegarde du modèle SVD entraîné ---\n",
        "from surprise import dump\n",
        "dump.dump('../data/svd_model.pkl', algo=algo_svd)\n",
        "print(\"\\nModèle SVD sauvegardé sous '../data/svd_model.pkl'.\")\n",
        "\n",
        "print(\"\\n--- Implémentation et évaluation des modèles de base terminées ! ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
